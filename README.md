Advanced Web Scraper with SQLite and CSV Output

An advanced web scraper built with Python that extracts data from multiple pages, stores the data in an SQLite database, and saves the output to a CSV file.

âœ¨ Features

Â· âœ… Supports data extraction from multiple pages (pagination)
Â· âœ… Extracts title, price, rating, and link (configurable)
Â· âœ… Saves data to SQLite database
Â· âœ… Saves data to CSV file
Â· âœ… Handles errors, timeouts, and retries
Â· âœ… Clean and structured code with professional folder organization

ğŸ“ Project Structure

```
advanced_scraper/
â”‚
â”œâ”€â”€ scraper.py          # Contains web scraping logic
â”œâ”€â”€ database.py         # Handles SQLite database connection and queries
â”œâ”€â”€ save_to_csv.py      # Saves scraped data to CSV file
â”œâ”€â”€ main.py             # Main script that runs the scraper
â””â”€â”€ requirements.txt    # Required Python libraries
```

ğŸš€ Installation

1. Clone the repository

```bash
git clone https://github.com/yourusername/advanced_scraper.git
cd advanced_scraper
```

2. Create a virtual environment (optional but recommended)

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies

```bash
pip install -r requirements.txt
```

ğŸ’» Usage

Run the scraper

```bash
python main.py
```

The scraper will fetch data from books.toscrape.com, extract product title, price, rating, and link, and save the data in two formats:

Â· SQLite database: products.db
Â· CSV file: products.csv

âš™ï¸ Customization

Scrape specific pages

Modify the run_scraper() function in main.py:

```python
run_scraper(start_page=1, end_page=5)  # Change page range as needed
```

Change extracted data

Modify the scrape_page() function in scraper.py.

ğŸ›¡ï¸ Error Handling

Â· Timeout: Retries if server response takes too long
Â· Retry: Up to 3 retries before skipping a page
Â· Invalid data: Skips items that cannot be extracted

ğŸ“Š Output

After running the scraper, the following files are generated:

Â· SQLite Database (products.db) â€” Contains all scraped product data
Â· CSV File (products.csv) â€” Contains all scraped product data in CSV format

Example CSV Output

```csv
title,price,rating,link
"A Light in the Attic","$51.77","Three","https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html"
"Sharp Objects","$47.75","Five","https://books.toscrape.com/catalogue/sharp-objects_999/index.html"
...
```

ğŸ¤ Contributing

Feel free to fork the repository, create issues, and submit pull requests. Contributions are welcome!

ğŸ“„ License

This project is licensed under the MIT License â€” see the LICENSE file for details.

ğŸ“ Support
For bug reports or feature suggestions:
Â·Create an Issue on GitHub   
Â·Username: MhdVesre00

â­ If you found this project useful, please give it a star on GitHub!
